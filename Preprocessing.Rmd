---
title: "R Notebook"
output: html_notebook
---
##Step o library and dependency
```{r}
# loading the required packages
install.packages("dplyr")
install.packages("tidyr")
install.packages("tm")
install.packages("corpus")
install.packages("e1071")
install.packages("wordcloud2")
install.packages("caret")
install.packages("tidytext")
install.packages("tokenizers")
install.packages("textclean")
install.packages("stopwords")
install.packages("purrr")
install.packages("tibble")
install.packages("ggwordcloud")
install.packages("scales")#for method date_format
install.packages("lubridate")#fro floor_date method
install.packages("textmineR")

library(textstem)
library(textmineR)
library(scales)
library(lubridate)
library(ggwordcloud)
library(tibble)
library(purrr)
library(textclean)
library(dplyr)
library(tidyr)
library(textreg)
library(tm)
library(writexl)
library(corpus)
library(e1071)
library(wordcloud2)
library(caret)
library(tidytext)
library(tokenizers)
```

##Step 1 Load Data
```{r}
rawData <- read.csv(file = "C:/Users/User/Documents/UNITS/Magistrale/Primo Anno/1Â°semestre/Machine Learning/SvevoLetters/VABENE.csv", sep = ";", header = TRUE, encoding = "Latin-1")
head(rawData)
dimensions <- dim(rawData)
rows <- dimensions[1]
columns <- dimensions[2]
```
##Step 2 pre processing
```{r}
LetterCorpusSection <- table(rawData$corpus)
Sections <- dim(LetterCorpusSection)
Sections
SectionNames <-levels(factor(rawData$corpus))
SectionNames
Languages<-levels(factor(rawData$mainLanguage))
Languages
letters_for_languages <- table(rawData$mainLanguage)
print(letters_for_languages)
```


##New section in which we perform preprocessing
```{r}

rawData$date <- gsub("00","01",rawData$date)#for unknown month or year we assume it is the first month and first year
rawData$date

#converting the data as a character into a Date object 
rawData$date <- as.Date(rawData$date,tryFormats = c("%d-%m-%Y", "%d/%m/%Y"))
#taking in account only the Italian letters because they are the vast majority in the dataset
rawData<- subset(rawData, mainLanguage=="ITA")
```
#creation of the dictionary of the lemmatization and see the frequency of noun and pronouns and verbs to choose which to remove from the corpus
```{r}
install.packages("udpipe")
library(udpipe)
ud_model <- udpipe_download_model(language = "italian")

ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = enc2utf8(rawData$text), doc_id = row.names(rawData))
x <- as.data.frame(x)

x$token_lower = tolower(x$token)
x$lemma_lower = tolower(x$lemma)
x <- subset(x, select = -c(token,lemma))
names(x)[names(x)=="token_lower"]<-"token" 
names(x)[names(x)=="lemma_lower"]<-"lemma"

lemmadictionary <- distinct(x[,c("token","lemma")],token, .keep_all = TRUE)

lemmatizeStringpersonal <- function (x){
  lemmatize_strings(x,dictionary = lemmadictionary)
}

install.packages("lattice")#for barchart
library(lattice)
#view the frequency of the most common terms find in the corpus
stats <- txt_freq(x$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "cadetblue", 
         main = "UPOS (Universal Parts of Speech)\n frequency of occurrence", 
         xlab = "Freq")
#noun
stats <- subset(x, upos %in% c("NOUN")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring nouns", xlab = "Freq")

#VERB
stats <- subset(x, upos %in% c("VERB")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring verbs", xlab = "Freq")

#PRON
stats <- subset(x, upos %in% c("PRON")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring pronouns", xlab = "Freq")

pronounstoremove <- as.character(x[x$upos %in% c("PRON"),c("lemma")])

#ADP
stats <- subset(x, upos %in% c("ADP")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring ADP", xlab = "Freq")

adptoremove <- as.character(x[x$upos %in% c("ADP"), c("lemma")])

#DET
stats <- subset(x, upos %in% c("DET")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring DET", xlab = "Freq")

dettoremove <- as.character(x[x$upos %in% c("DET"), c("lemma")])
```

##create the corpus composed only by the text
```{r}
myCorpus <- textclean::replace_non_ascii(rawData$text) %>%VectorSource()%>%VCorpus()
#remove numbers
myCorpus<- tm_map(myCorpus, content_transformer(removeNumbers))
#remove punctuation
myRemovePunctuation <- function(x){gsub("([<>])|[[:punct:]]","",x)}
myCorpus <- tm_map(myCorpus,content_transformer(myRemovePunctuation))
#to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#remove stopwords
myStopwords <- c(stopwords("it"),stopwords("en"),stopwords("fr"),stopwords("de"))

chunk <- 500
n <- length(myStopwords)
r <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
d <- split(myStopwords,r)

for (i in 1:length(d)) {
  myCorpus <- tm_map(myCorpus, removeWords, c(paste(d[[i]])))
}

#stripe whitespaces
myCorpus<- tm_map(myCorpus, stripWhitespace)
#myCorpus[[1]]$content

#lemmatize string
myCorpus <- tm_map(myCorpus, lemmatizeStringpersonal)

#remove nouns and so on
myStopwords2 <- c(pronounstoremove,adptoremove,dettoremove)
chunk <- 500
n <- length(myStopwords2)
r <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
d <- split(myStopwords,r)

for (i in 1:length(d)) {
  myCorpus <- tm_map(myCorpus, removeWords, c(paste(d[[i]])))
}

#plain text transformation
myCorpus <- tm_map(myCorpus, removeWords, c("NA"))
myCorpus <- tm_map(myCorpus, PlainTextDocument)
#myCorpus[[1]]$content
```

##creation of the different dtm matrix
```{r}
createdtmtf <- function(pruned){
  bigramTokenizer <- 
    function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = "_"), use.names = FALSE)
trigramTokenizer <- 
    function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = "_"), use.names = FALSE)
#DTM  
#unigramdtm
unidtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths=c(2,Inf)))
if(pruned ==TRUE){
  unidtm <- removeSparseTerms(unidtm, 0.9999)
  terms <- findFreqTerms(unidtm, lowfreq = 3)
  unidtm <- unidtm[,terms]
  }# elimino i termini che non compaiono dall'90% in su dei dicumenti}
#bigramdtm
  bidtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = bigramTokenizer,wordLengths=c(2,Inf) ))
  if(pruned == TRUE){
  bidtm <- removeSparseTerms(bidtm, 0.99999)
  terms <- findFreqTerms(bidtm, lowfreq = 3)
  bidtm <- bidtm[,terms]}
  
#trigramdtm
  tridtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = trigramTokenizer,wordLengths=c(2,Inf) ))
  if(pruned ==TRUE){
  unidtm <- removeSparseTerms(unidtm, 0.99999)
  terms <- findFreqTerms(tridtm, lowfreq = 3)
  tridtm <- tridtm[,terms]
  }
  install.packages("wordcloud")
  library(wordcloud)
#inspect union and bigrams
unionunibitridtm <- cbind(unidtm,bidtm,tridtm)
attributes(unionunibitridtm) <- attributes(unidtm)
m <- as.matrix(unionunibitridtm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(freq=v,word = names(v))
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=400, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
return(unionunibitridtm)
}

dtmtf <- createdtmtf(FALSE)
dtmtfpruned <- createdtmtf(TRUE)

```
##Evaluation of the LDA model to find the optimal number of arguments, preparing the data frame related with coherenc and number of topics
```{r}
graphs_data <- function(matrix){
jaccard_similarity<- function(topic1,topic2){
  intersection = length(intersect(topic1,topic2))
  union = length(topic1)+length(topic2)- intersection
  return (intersection/union)
}
dtm_lda <- Matrix::Matrix(as.matrix(matrix), sparse = T)
coherence = c()
similarity = c()
kvalues = seq(2,10,by=1)
for(k in kvalues){
  set.seed(123)
  m <- FitLdaModel(dtm =dtm_lda,
                        k = k, 
                        iterations = 500,
                        burnin = 400, 
                        calc_coherence = T
                        )
  letters_word_topic_iterative <- GetTopTerms(m$phi,30)%>%as.data.frame()%>%set_names(paste("topic",1:k))
  sims = c()
  for(t1 in 1:(k-1)){
    for(t2 in (t1+1):k ){
      topic1 <- as.vector(letters_word_topic_iterative[,c(t1)])
      topic2 <- as.vector(letters_word_topic_iterative[,c(t2)])
      sims <- c(sims, jaccard_similarity(topic1,topic2))                    
    }
  }
  similaritymean <- mean(sims)
  similarity <- c(similarity, similaritymean)
  
  cmean <- mean(m$coherence)
  coherence <- c(coherence, cmean)
}
data_graph_k_C_s <- data.frame(coherence,similarity,kvalues)
return(data_graph_k_C_s)
}
dftfNtopic <- graphs_data(dtmtf)
dftfNtopicPruned <- graphs_data(dtmtfpruned)
```

##visualizaation of coherance with teh number of topics
```{r}
plot <- function(data_graph_k_C_s){
data_graph_k_C_s <- data_graph_k_C_s %>%
  rowwise()%>%
  mutate(DiffCS = coherence - similarity)
##1K TOPICS SELECTOR-----------------------
#ideal_k = which(data_graph_k_C_s$coherence == max(coherence) & data_graph_k_C_s$similarity == min(similarity))
##2K TOPICS SELECTOR--------------------------
#ideal_k = as.integer(data_graph_k_C_s[data_graph_k_C_s$DiffCS == max(data_graph_k_C_s$DiffCS), "kvalues"])
##3K TOPICS SELECTOR------------------------
#Take the best 5 potentially k values and from the 5 we consider only 1 with the less kvalues
ideal_5_keys <- head(data_graph_k_C_s[order(-data_graph_k_C_s$DiffCS),],3)
ideal_k <- as.integer(ideal_5_keys[ideal_5_keys$kvalues == min(ideal_5_keys$kvalues), "kvalues"])
print(data_graph_k_C_s%>%
  gather(key,value,coherence,similarity)%>%
  ggplot(aes(x=kvalues, y=value, colour=key))+
  geom_line()+
  geom_vline(xintercept = ideal_k,color="red"))
#ideal k 13/12/2021
#ggplot(data=data_graph_k_C, aes(x=kvalues,y=coherence)) +
 # geom_line() +
  #geom_point()+
  #geom_vline(xintercept = ideal_k,color="red")+
  #scale_x_continuous(breaks = seq(0, 10, 1)) +
  #labs(x = "Number of Topics", y = "Coherence", title = "Coherence Score over Number of Topics") +
  #theme_minimal() +
  #theme(panel.grid.minor = element_blank())
 return(ideal_k)
}
Ntopic <- plot(dftfNtopic)
Ntopicpruned <- plot(dftfNtopicPruned)
```
##Topic modelling with LDA LAtent Dirichlet Allocation
```{r}
install.packages("topicmodels")
library(topicmodels)
dtm_lda <- Matrix::Matrix(as.matrix(dtmtfpruned), sparse = T)
set.seed(123)
lda <- FitLdaModel(dtm =dtm_lda,
                        k = 5, 
                        iterations = 500,
                        burnin = 400, 
                        calc_coherence = T
                        )
install.packages("topicmodels")
library(topicmodels)
set.seed(123)
#lda_tfidf <- FitLdaModel(dtm =dtm_lda_tfidf,
 #                       k = 2, 
  #                      iterations = 500,
   #                     burnin = 400, 
    #                    calc_coherence = T
     #                   )
```
##Below are some important attribute acquired from the LDA Model:

phi : Posterior per-topic-per-word probabilities
theta : Posterior per-document-per-topic probabilities
alpha : Prior per-document-per-topic probabilities
beta : Prior per-document-per-topic probabilities
coherence : The probabilistic coherence of each topic

If a term has a high value of theta, it has a high probability of that term being generated from that topic. This also indicates that the term has a high association toward a certain topic.
```{r}
lda$phi%>% rowSums()
letters_word_topic <- GetTopTerms(lda$phi,10)%>% as.data.frame()%>%set_names(paste("topic",1:5))
View(letters_word_topic)

#lda_tfidf$phi%>% rowSums()
#letters_word_topic_tfidf <- GetTopTerms(lda_tfidf$phi,10)%>% as.data.frame()%>%set_names(paste("topic",1:2))
#View(letters_word_topic_tfidf)
```


##grphical way to reppresents topic with related words using word cloud
```{r}
letters_word_topic %>% 
   rownames_to_column("id") %>%
   mutate(id = as.numeric(id)) %>% 
   pivot_longer(-id, names_to = "topic", values_to = "term") %>% 
   ggplot(aes(label = term, size = rev(id), color = topic, alpha = rev(id))) +
   geom_text_wordcloud(seed = 123) +
   geom_text_wordcloud_area(rm_outside = TRUE)+
   facet_wrap(~topic, scales = "free") +
   scale_alpha_continuous(range = c(0.4, 1)) +
   #scale_color_manual(values = c( "dodgerblue4", "firebrick4", "darkgreen")) +
   theme_minimal() +
   theme(strip.background = element_rect(fill = "firebrick"),
         strip.text.x = element_text(colour = "white"))
```

#Top topic for each document wit thew respective yera
```{r}
docThetaTopic <- lda$theta %>% 
   as.data.frame()
colnames(docThetaTopic) <- c(1,2,3,4,5)

docTopicClassificationDf <- as.data.frame(cbind(id=row.names(rawData), topic= apply(docThetaTopic,1,function(x)names(docThetaTopic)[which.max(x)]), year = rawData$year))

lettersTopic <- table(docTopicClassificationDf$topic)
print(lettersTopic)
```

#From teh data frame toptopics add the date for each document to plot the occourences of the topic over the time
```{r}
#doctopicclassification["date"] <- rawData$date
#doctopicclassification["year"] <- rawData$year
daterange <- range(rawData$date, na.rm = TRUE)
date_1 <- as.Date(daterange[1])
date_2 <- as.Date(daterange[2])

yearsvector <- rownames(table(docTopicClassificationDf$year))

topics <- c(1,2,3,4,5)
datalistTopicYearDf <- data.frame(year = integer(), numberTopicsYear= integer(),topics =integer())

for(year in yearsvector){
  docTopicClassification_year <- docTopicClassificationDf[docTopicClassificationDf$year == year,]
    
    #count the number of different topics for the current year
    numberTopicsYear <- table(docTopicClassification_year$topic)
    print(numberTopicsYear)
    topics <- row.names(numberTopicsYear)
  
    
    letterTopicYearDf <- data.frame(year,numberTopicsYear)%>%rename(topics =  Var1,numberTopicsYear = Freq)

    datalistTopicYearDf<-rbind(datalistTopicYearDf, letterTopicYearDf)
    
 }

#convert the year into a date object  
datalistTopicYearDf$year = as.Date(as.character(datalistTopicYearDf$year), format = "%Y")

#plot the graph of the number of letters per topic during each year 
print(ggplot(datalistTopicYearDf, aes(x = year %m-% months(12) , y = numberTopicsYear, fill = topics, group = topics, colour = topics)) +scale_y_continuous(name = "count")+ geom_line()+geom_point()+facet_wrap(~ factor(topics, levels = c(1,2,3,4,5)))+ theme(legend.position = "none") + xlab("year"))




#OLD STUFF##########################################################################
#}
    
#    }

#years <- seq(from= date_1, to=date_2, by="year")
#years <- length(years)-1
#numberofyears <- nrow(table(rawData$year))#geom_freqpoly(bins = numberofyears )
#ggplot(doctopicclassification,mapping = aes(x= year, fill = topic, group=topic, colour = topic))+
 # geom_bar()+
  #geom_freqpoly(bins = years) + scale_x_continuous(minor_breaks = seq(1885,1930,1))
  #facet_wrap(facets = vars(topic)) + 
#theme(legend.position = "none")
```
##we add the column of the receivers to our data frame
```{r}
install.packages("stringr")
library(stringr)
docTopicClassificationDf$correspondence<- rawData$corpus%>%str_replace_all(c("Svevo"="","Schmitz"=""))


#all the receivers with less than 5 letters are marked as others
correspondenceLessThan5 <- as.data.frame(table(docTopicClassificationDf$correspondence)) %>% rename(correspondence = Var1, count = Freq)%>%subset(count<=5)

correspondenceLessThan5 <- as.vector(correspondenceLessThan5$correspondence)

#correspondenceLessThan5 <- subset(numberLettersCorrespondence, numberLettersCorrespondence$count <=5 )

#correspondenceLessThan5 <- as.vector(correspondenceLessThan5$correspondence)

docTopicClassificationDf$correspondence[docTopicClassificationDf$correspondence %in% correspondenceLessThan5]<- "Others"
```


```{r}
install.packages("scales")
install.packages("ggrepel")
install.packages("randomcoloR")
library(ggrepel)
library(scales)
library(dplyr)
library(randomcoloR)
set.seed(2)
pal <- randomColor(count = length(unique(docTopicClassificationDf$correspondence)))
pal = setNames(pal, unique(docTopicClassificationDf$correspondence))

for(i in 1:5){
  docTopicClassification_partial <- docTopicClassificationDf[docTopicClassificationDf$topic == i,]   
  
correspondencePercentageTopicDf <- as.data.frame(table(docTopicClassification_partial$correspondence)) %>% subset(Freq !=0) %>% mutate(perc = Freq /sum(Freq) * 100 ) %>% mutate(labels = percent(perc, scale = 1, accuracy = 0.01)) %>% rename(correspondence =  Var1)

dflabel <- correspondencePercentageTopicDf %>% mutate(csum = rev(cumsum(rev(perc))), 
         pos = perc/2 + lead(csum, 1),
         pos = if_else(is.na(pos), perc/2, pos))
  
print(ggplot(correspondencePercentageTopicDf, aes(x = "" , y = perc, fill = correspondence)) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_manual(values = pal, guide="none")+
  geom_label_repel(data = dflabel,
                   aes(y = pos, label = paste(labels, " \n",correspondence)),
                   size =2.5 , nudge_x = 4, show.legend = FALSE) +
  theme_void()+
  ggtitle(paste0("Topic ", i)))
}
```
#Triyng alluvial plot
```{r}
#install.packages("ggalluvial")
#library(ggalluvial)
#doctopicclassification$year <- rawData$year
#doctopicclassification_crouped <- doctopicclassification%>%group_by(topic,receivers,year)%>%  tally()
#ggplot(data = doctopicclassification_crouped,
#       aes(axis1 = topic, axis2 = year , axis3 = receivers, y = n)) +
#  geom_alluvium(aes(fill = topic), curve_type = "quintic") +
#  geom_stratum(aes(fill = topic)) +
#  geom_text(stat = "stratum",
#            aes(label = after_stat(stratum))) +
#  theme_void()
```

#sentiment anlayisis
```{r}
install.packages("syuzhet")
library(syuzhet)

#conversion of the clenaed corpus into a data frame
myCorpusDf <- data.frame(text = unlist(sapply(myCorpus,`[`, "content")), stringsAsFactors = FALSE)
myCorpusDf <- data.frame(text = myCorpusDf[!(myCorpusDf$text == ""),])

lettersVector <- as.character(myCorpusDf$text)
#obtaining sentiment scores
sentimentEmotionsScores <- get_nrc_sentiment(lettersVector, language = "italian")


#combine text sentiment, datae and correspondances columns
senttimentEmotionsScoresDf <- cbind(docTopicClassificationDf, lettersVector,sentimentEmotionsScores)

#barplot for sentiment
overallSentimentEmotion <- subset(sentimentEmotionsScores, select = -c(9,10))
barplot(colSums(overallSentimentEmotion), col = rainbow(8), ylab = "Count", main = "Overall sentiment analysis", las = 2)
```

#plot the graph to see the sentiment analysis for each topic
```{r}
emotions<-c("anger","anticipation","disgust","fear","joy","sadness","surprise","trust")
sentiment <- c("negative","positive")

plotSentimentAanalysisOverTopic <- function(SentimentEmotionVector)
for(t in 1:5){
  sentimentdf_partial <- sentimentEmotionsScoresDf[sentimentEmotionsScoresDf$topic == t,] 
  
  WordsForSentiment = c()
  
  for(e in SentimentEmotionVector ){
    
    value <- sum(sentimentdf_partial[,e])
    WordsForSentiment <- c(WordsForSentiment, value)
  }
  
  names(WordsForSentiment)  <- SentimentEmotionVector
 
  totalSentimentWordTopic <- sum(WordsForSentiment)
  
  relativePercentage <- c()
  
  for(e in SentimentEmotionVector){
    
    value <- WordsForSentiment[[e]]
    relativePercentage <- c(relativePercentage, value/totalSentimentWordTopic *100)
  }
  
  SentimentEmotiondf <- data.frame(relativePercentage, SentimentEmotionVector)
  
  SentimentEmotiondf<- SentimentEmotiondf %>% mutate(labels = percent(relativePercentage, scale = 1, accuracy = 0.01))
  
  print(ggplot(SentimentEmotiondf, aes(x = "" , y = relativePercentage, fill = SentimentEmotionVector)) +
  geom_col(width = 1, color = 1) +
  geom_text(aes(label = labels), position = position_stack(vjust = 0.5))+
  coord_polar(theta = "y") +
  theme_void()+
  ggtitle(paste0("Topic ", t)))
}

plotSentimentAanalysisOverTopic(emotions) 
plotSentimentAanalysisOverTopic(sentiment)



```

#plot the proprotion of emotions during the years
```{r}

plotSentimentAanalysisOverTime <- function(SentimentEmotionVector, facet_rows, facet_col, wrap){

  #set the data frame to collect the data for each year
  yearsvector <- rownames(table(sentimentEmotionsScoresDf$year))
  datalist <- data.frame(relativePercentage = integer(), SentimentEmotionVector = character(),year =integer())
  
  #for each year calculate the percentage of each emotion and add it to the datalist dataframe
  for (year in yearsvector){
    sentimentdf_partial_year <- sentimentEmotionsScoresDf[sentimentEmotionsScoresDf$year == year,]
    
    WordsForSentiment = c()
    
    #count the number of words for each sentiment_emotion
    for(e in SentimentEmotionVector ){
      
      value <- sum(sentimentdf_partial_year[,e])
      WordsForSentiment <- c(WordsForSentiment, value)
    }
    
    names(WordsForSentiment)  <- SentimentEmotionVector
    
    #count the total number of words for all the sentiment_emotions
    TotalSentimentEmotionWords <- sum(WordsForSentiment)
    
    relativePercentage <- c()
    
    #for each sentiment_emotion caluclate the percentage
    for(e in SentimentEmotionVector){
      
      value <- WordsForSentiment[[e]]
      relativePercentage <- c(relativePercentage, value/TotalSentimentEmotionWords *100)
    }
    
    SentimentEmotiondf <- data.frame(relativePercentage, SentimentEmotionVector, year)
    datalist<-rbind(datalist, SentimentEmotiondf)
    
  }
  
  datalist$year = as.Date(as.character(datalist$year), format = "%Y")
 
  if(wrap == TRUE){ 
  print(ggplot(datalist, aes(x = year %m-% months(12) , y = relativePercentage, fill = SentimentEmotionVector, group = SentimentEmotionVector, colour = SentimentEmotionVector)) +scale_y_continuous(name = "Percentage (%)")+geom_line()+geom_point()+facet_wrap(~ SentimentEmotionVector, nrow = facet_rows, ncol = facet_col)+xlab("year")+ theme(legend.position = "none"))
  }
  else{
    print(ggplot(datalist, aes(x = year %m-% months(12) , y = relativePercentage, fill = SentimentEmotionVector, group = SentimentEmotionVector, colour = SentimentEmotionVector)) +scale_y_continuous(name = "Percentage (%)")+geom_line()+geom_point()+xlab("year")+ theme(legend.position = "none"))
  }
}

plotSentimentAanalysisOverTime(emotions,2,4,TRUE)
plotSentimentAanalysisOverTime(sentiment,2,1,FALSE)


```












Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
