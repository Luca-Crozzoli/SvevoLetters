---
title: "R Notebook"
output: html_notebook
---
##Step o library and dependency
```{r}
# loading the required packages
install.packages("dplyr")
install.packages("tidyr")
install.packages("tm")
install.packages("corpus")
install.packages("e1071")
install.packages("wordcloud2")
install.packages("caret")
install.packages("tidytext")
install.packages("tokenizers")
install.packages("textclean")
install.packages("stopwords")
install.packages("purrr")
install.packages("tibble")
install.packages("ggwordcloud")
install.packages("scales")#for method date_format
install.packages("lubridate")#fro floor_date method
install.packages("textmineR")

library(textstem)
library(textmineR)
library(scales)
library(lubridate)
library(ggwordcloud)
library(tibble)
library(purrr)
library(textclean)
library(dplyr)
library(tidyr)
library(textreg)
library(tm)
library(writexl)
library(corpus)
library(e1071)
library(wordcloud2)
library(caret)
library(tidytext)
library(tokenizers)
```

##Step 1 Load Data
```{r}
rawData <- read.csv(file = "C:/Users/User/Documents/UNITS/Magistrale/Primo Anno/1Â°semestre/Machine Learning/SvevoLetters/VABENE.csv", sep = ";", header = TRUE, encoding = "Latin-1")
head(rawData)
dimensions <- dim(rawData)
rows <- dimensions[1]
columns <- dimensions[2]
```
##Step 2 pre processing
```{r}
LetterCorpusSection <- table(rawData$corpus)
Sections <- dim(LetterCorpusSection)
Sections
SectionNames <-levels(factor(rawData$corpus))
SectionNames
Languages<-levels(factor(rawData$mainLanguage))
Languages
letters_for_languages <- table(rawData$mainLanguage)
print(letters_for_languages)
```


##New section in which we perform preprocessing
```{r}

rawData$date <- gsub("00","01",rawData$date)#for unknown month or year we assume it is the first month and first year
rawData$date

#converting the data as a character into a Date object 
rawData$date <- as.Date(rawData$date,tryFormats = c("%d-%m-%Y", "%d/%m/%Y"))
#taking in account only the Italian letters because they are the vast majority in the dataset
rawData<- subset(rawData, mainLanguage=="ITA")
```
#creation of the dictionary of the lemmatization and see the frequency of noun and pronouns and verbs to choose which to remove from the corpus
```{r}
install.packages("udpipe")
library(udpipe)
ud_model <- udpipe_download_model(language = "italian")

ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = enc2utf8(rawData$text), doc_id = row.names(rawData))
x <- as.data.frame(x)

x$token_lower = tolower(x$token)
x$lemma_lower = tolower(x$lemma)
x <- subset(x, select = -c(token,lemma))
names(x)[names(x)=="token_lower"]<-"token" 
names(x)[names(x)=="lemma_lower"]<-"lemma"

lemmadictionary <- distinct(x[,c("token","lemma")],token, .keep_all = TRUE)

lemmatizeStringpersonal <- function (x){
  lemmatize_strings(x,dictionary = lemmadictionary)
}

install.packages("lattice")#for barchart
library(lattice)
#view the frequency of the most common terms find in the corpus
stats <- txt_freq(x$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "cadetblue", 
         main = "UPOS (Universal Parts of Speech)\n frequency of occurrence", 
         xlab = "Freq")
#noun
stats <- subset(x, upos %in% c("NOUN")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring nouns", xlab = "Freq")

#VERB
stats <- subset(x, upos %in% c("VERB")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring verbs", xlab = "Freq")

#PRON
stats <- subset(x, upos %in% c("PRON")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring pronouns", xlab = "Freq")

pronounstoremove <- as.character(x[x$upos %in% c("PRON"),c("lemma")])

#ADP
stats <- subset(x, upos %in% c("ADP")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring ADP", xlab = "Freq")

adptoremove <- as.character(x[x$upos %in% c("ADP"), c("lemma")])

#DET
stats <- subset(x, upos %in% c("DET")) 
stats <- txt_freq(stats$lemma)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring DET", xlab = "Freq")

dettoremove <- as.character(x[x$upos %in% c("DET"), c("lemma")])
```

##create the corpus composed only by the text
```{r}
myCorpus <- textclean::replace_non_ascii(rawData$text) %>%VectorSource()%>%VCorpus()
#remove numbers
myCorpus<- tm_map(myCorpus, content_transformer(removeNumbers))
#remove punctuation
myRemovePunctuation <- function(x){gsub("([<>])|[[:punct:]]","",x)}
myCorpus <- tm_map(myCorpus,content_transformer(myRemovePunctuation))
#to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#remove stopwords
myStopwords <- c(stopwords("it"),stopwords("en"),stopwords("fr"),stopwords("de"))

chunk <- 500
n <- length(myStopwords)
r <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
d <- split(myStopwords,r)

for (i in 1:length(d)) {
  myCorpus <- tm_map(myCorpus, removeWords, c(paste(d[[i]])))
}

#stripe whitespaces
myCorpus<- tm_map(myCorpus, stripWhitespace)
#myCorpus[[1]]$content

#lemmatize string
myCorpus <- tm_map(myCorpus, lemmatizeStringpersonal)

#remove nouns and so on
myStopwords2 <- c(pronounstoremove,adptoremove,dettoremove)
chunk <- 500
n <- length(myStopwords2)
r <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
d <- split(myStopwords,r)

for (i in 1:length(d)) {
  myCorpus <- tm_map(myCorpus, removeWords, c(paste(d[[i]])))
}

#plain text transformation
myCorpus <- tm_map(myCorpus, removeWords, c("NA"))
myCorpus <- tm_map(myCorpus, PlainTextDocument)
#myCorpus[[1]]$content
```

##creation of the different dtm matrix
```{r}
createdtmtf <- function(pruned){
  bigramTokenizer <- 
    function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = "_"), use.names = FALSE)
trigramTokenizer <- 
    function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = "_"), use.names = FALSE)
#DTM  
#unigramdtm
unidtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths=c(2,Inf)))
if(pruned ==TRUE){
  unidtm <- removeSparseTerms(unidtm, 0.9999)
  terms <- findFreqTerms(unidtm, lowfreq = 3)
  unidtm <- unidtm[,terms]
  }# elimino i termini che non compaiono dall'90% in su dei dicumenti}
#bigramdtm
  bidtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = bigramTokenizer,wordLengths=c(2,Inf) ))
  if(pruned == TRUE){
  bidtm <- removeSparseTerms(bidtm, 0.99999)
  terms <- findFreqTerms(bidtm, lowfreq = 3)
  bidtm <- bidtm[,terms]}
  
#trigramdtm
  tridtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = trigramTokenizer,wordLengths=c(2,Inf) ))
  if(pruned ==TRUE){
  unidtm <- removeSparseTerms(unidtm, 0.99999)
  terms <- findFreqTerms(tridtm, lowfreq = 3)
  tridtm <- tridtm[,terms]
  }
  install.packages("wordcloud")
  library(wordcloud)
#inspect union and bigrams
unionunibitridtm <- cbind(unidtm,bidtm,tridtm)
attributes(unionunibitridtm) <- attributes(unidtm)
m <- as.matrix(unionunibitridtm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(freq=v,word = names(v))
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=400, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
return(unionunibitridtm)
}

dtmtf <- createdtmtf(FALSE)
dtmtfpruned <- createdtmtf(TRUE)

```
##Evaluation of the LDA model to find the optimal number of arguments, preparing the data frame related with coherenc and number of topics
```{r}
graphs_data <- function(matrix){
jaccard_similarity<- function(topic1,topic2){
  intersection = length(intersect(topic1,topic2))
  union = length(topic1)+length(topic2)- intersection
  return (intersection/union)
}
dtm_lda <- Matrix::Matrix(as.matrix(matrix), sparse = T)
coherence = c()
similarity = c()
kvalues = seq(2,10,by=1)
for(k in kvalues){
  set.seed(123)
  m <- FitLdaModel(dtm =dtm_lda,
                        k = k, 
                        iterations = 500,
                        burnin = 400, 
                        calc_coherence = T
                        )
  letters_word_topic_iterative <- GetTopTerms(m$phi,30)%>%as.data.frame()%>%set_names(paste("topic",1:k))
  sims = c()
  for(t1 in 1:(k-1)){
    for(t2 in (t1+1):k ){
      topic1 <- as.vector(letters_word_topic_iterative[,c(t1)])
      topic2 <- as.vector(letters_word_topic_iterative[,c(t2)])
      sims <- c(sims, jaccard_similarity(topic1,topic2))                    
    }
  }
  similaritymean <- mean(sims)
  similarity <- c(similarity, similaritymean)
  
  cmean <- mean(m$coherence)
  coherence <- c(coherence, cmean)
}
data_graph_k_C_s <- data.frame(coherence,similarity,kvalues)
return(data_graph_k_C_s)
}
dftfNtopic <- graphs_data(dtmtf)
dftfNtopicPruned <- graphs_data(dtmtfpruned)
```

##visualizaation of coherance with teh number of topics
```{r}
plot <- function(data_graph_k_C_s){
data_graph_k_C_s <- data_graph_k_C_s %>%
  rowwise()%>%
  mutate(DiffCS = coherence - similarity)
##1K TOPICS SELECTOR-----------------------
#ideal_k = which(data_graph_k_C_s$coherence == max(coherence) & data_graph_k_C_s$similarity == min(similarity))
##2K TOPICS SELECTOR--------------------------
#ideal_k = as.integer(data_graph_k_C_s[data_graph_k_C_s$DiffCS == max(data_graph_k_C_s$DiffCS), "kvalues"])
##3K TOPICS SELECTOR------------------------
#Take the best 5 potentially k values and from the 5 we consider only 1 with the less kvalues
ideal_5_keys <- head(data_graph_k_C_s[order(-data_graph_k_C_s$DiffCS),],3)
ideal_k <- as.integer(ideal_5_keys[ideal_5_keys$kvalues == min(ideal_5_keys$kvalues), "kvalues"])
print(data_graph_k_C_s%>%
  gather(key,value,coherence,similarity)%>%
  ggplot(aes(x=kvalues, y=value, colour=key))+
  geom_line()+
  geom_vline(xintercept = ideal_k,color="red"))
#ideal k 13/12/2021
#ggplot(data=data_graph_k_C, aes(x=kvalues,y=coherence)) +
 # geom_line() +
  #geom_point()+
  #geom_vline(xintercept = ideal_k,color="red")+
  #scale_x_continuous(breaks = seq(0, 10, 1)) +
  #labs(x = "Number of Topics", y = "Coherence", title = "Coherence Score over Number of Topics") +
  #theme_minimal() +
  #theme(panel.grid.minor = element_blank())
 return(ideal_k)
}
Ntopic <- plot(dftfNtopic)
Ntopicpruned <- plot(dftfNtopicPruned)
```
##Topic modelling with LDA LAtent Dirichlet Allocation
```{r}
install.packages("topicmodels")
library(topicmodels)
dtm_lda <- Matrix::Matrix(as.matrix(dtmtfpruned), sparse = T)
set.seed(123)
lda <- FitLdaModel(dtm =dtm_lda,
                        k = 5, 
                        iterations = 500,
                        burnin = 400, 
                        calc_coherence = T
                        )
install.packages("topicmodels")
library(topicmodels)
set.seed(123)
#lda_tfidf <- FitLdaModel(dtm =dtm_lda_tfidf,
 #                       k = 2, 
  #                      iterations = 500,
   #                     burnin = 400, 
    #                    calc_coherence = T
     #                   )
```
##Below are some important attribute acquired from the LDA Model:

phi : Posterior per-topic-per-word probabilities
theta : Posterior per-document-per-topic probabilities
alpha : Prior per-document-per-topic probabilities
beta : Prior per-document-per-topic probabilities
coherence : The probabilistic coherence of each topic

If a term has a high value of theta, it has a high probability of that term being generated from that topic. This also indicates that the term has a high association toward a certain topic.
```{r}
lda$phi%>% rowSums()
letters_word_topic <- GetTopTerms(lda$phi,10)%>% as.data.frame()%>%set_names(paste("topic",1:5))
View(letters_word_topic)

#lda_tfidf$phi%>% rowSums()
#letters_word_topic_tfidf <- GetTopTerms(lda_tfidf$phi,10)%>% as.data.frame()%>%set_names(paste("topic",1:2))
#View(letters_word_topic_tfidf)
```


##grphical way to reppresents topic with related words using word cloud
```{r}
letters_word_topic %>% 
   rownames_to_column("id") %>%
   mutate(id = as.numeric(id)) %>% 
   pivot_longer(-id, names_to = "topic", values_to = "term") %>% 
   ggplot(aes(label = term, size = rev(id), color = topic, alpha = rev(id))) +
   geom_text_wordcloud(seed = 123) +
   geom_text_wordcloud_area(rm_outside = TRUE)+
   facet_wrap(~topic, scales = "free") +
   scale_alpha_continuous(range = c(0.4, 1)) +
   #scale_color_manual(values = c( "dodgerblue4", "firebrick4", "darkgreen")) +
   theme_minimal() +
   theme(strip.background = element_rect(fill = "firebrick"),
         strip.text.x = element_text(colour = "white"))
```

#most related document for topic 3 we use the theta parameter from the lda
```{r}
#letters_doc_topic <- lda_news$theta %>% 
#   as.data.frame() %>% 
#   rownames_to_column("id")#%>%
#   #mutate(id = as.numeric(id))
#letters_doc_topic %>% 
#   arrange(desc(t_3))%>%
#   left_join(rawData%>%
#               mutate(n=as.character(n))%>%
#               select(n,corpus,date,sender,recipient,text), 
#               by=c("id"= "n"))%>% 
#  select(id,date,sender,recipient,t_3)%>%
#  view()
```

#Top topic for each document
```{r}
doctoptopic <- lda$theta %>% 
   as.data.frame()
colnames(doctoptopic) <- c(1,2,3,4,5)
doctopicclassification <- as.data.frame(cbind(id=row.names(rawData), topic= apply(doctoptopic,1,function(x)names(doctoptopic)[which.max(x)])))
mostrelvanttopicinthecorpus <- table(doctopicclassification$topic)
print(mostrelvanttopicinthecorpus)
```

#From teh data frame toptopics add the date for each document to plot the occourences of the topic voer the time
```{r}
doctopicclassification["date"] <- rawData$date
daterange <- range(rawData$date, na.rm = TRUE)
numberofyears <- nrow(table(rawData$year))#geom_freqpoly(bins = numberofyears )
ggplot(doctopicclassification,mapping = aes(x= date, fill = topic, group=topic, colour = topic))+ geom_freqpoly() + #facet_wrap(facets = vars(topic)) + 
theme(legend.position = "none")
```
##we add the column of the receivers to our data frame
```{r}
install.packages("stringr")
library(stringr)
doctopicclassification$categorie_sender_receiver<- rawData$corpus
doctopicclassification$receivers <- str_replace_all(doctopicclassification$categorie_sender_receiver,"Svevo","")
doctopicclassification$receivers <- str_replace_all(doctopicclassification$receivers,"Schmitz","")

#all the receivers with less than 5 letters are marked as others
numberletters_receiver <- table(doctopicclassification$receivers)
print(numberletters_receiver)
numberletters_receiver <- as.data.frame(numberletters_receiver)
numberletters_receiver <- subset(numberletters_receiver, numberletters_receiver$Freq <=5 )

receiverlessthan5 <- as.vector(numberletters_receiver$Var1)

doctopicclassification$receivers[doctopicclassification$receivers %in% receiverlessthan5]<- "Others"
```


```{r}
install.packages("scales")
install.packages("ggrepel")
install.packages("randomcoloR")
library(ggrepel)
library(scales)
library(dplyr)
library(randomcoloR)
set.seed(2)
pal <- randomColor(count = length(unique(doctopicclassification$receivers)))
pal = setNames(pal, unique(doctopicclassification$receivers))

for(i in 1:5){
  doctopicclassification_partial <- doctopicclassification[doctopicclassification$topic == i,]   
  
dfpartial_topic <- as.data.frame(table(doctopicclassification_partial$receivers)) %>% subset(Freq !=0) %>% mutate(perc = Freq /sum(Freq) * 100 ) %>% mutate(labels = percent(perc, scale = 1, accuracy = 0.01)) %>% rename(receivers =  Var1)

dflabel <- dfpartial_topic %>% mutate(csum = rev(cumsum(rev(perc))), 
         pos = perc/2 + lead(csum, 1),
         pos = if_else(is.na(pos), perc/2, pos))
  
print(ggplot(dfpartial_topic, aes(x = "" , y = perc, fill = receivers)) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_manual(values = pal, guide="none")+
  geom_label_repel(data = dflabel,
                   aes(y = pos, label = paste(labels, " \n",receivers)),
                   size =2.5 , nudge_x = 4, show.legend = FALSE) +
  theme_void()+
  ggtitle(paste0("Topic ", i)))
}
```
#Triyng alluvial plot
```{r}
install.packages("ggalluvial")
library(ggalluvial)
doctopicclassification$year <- rawData$year
doctopicclassification_crouped <- doctopicclassification%>%group_by(topic,receivers,year)%>%  tally()
ggplot(data = doctopicclassification_crouped,
       aes(axis1 = topic, axis2 = year , axis3 = receivers, y = n)) +
  geom_alluvium(aes(fill = topic), curve_type = "quintic") +
  geom_stratum(aes(fill = topic)) +
  geom_text(stat = "stratum",
            aes(label = after_stat(stratum))) +
  theme_void()
```

#sentiment anlayisis
```{r}
install.packages("syuzhet")
library(syuzhet)

dfcorpus <- data.frame(text = unlist(sapply(myCorpus,`[`, "content")), stringsAsFactors = FALSE)
dfcorpus2 <- data.frame( text = dfcorpus[!(dfcorpus$text == ""),])

vector <- as.character(dfcorpus2$text)
#obtaining sentiment scores
sentimentscores <- get_nrc_sentiment(vector, language = "italian")
#remove the positive and negative columns
sentimentscores[9:10] <- NULL 

#combine text sentiment, datae and correspondances columns
sentimentdf <- cbind(doctopicclassification, vector,sentimentscores)

#barplot for sentiment
barplot(colSums(sentimentscores), col = rainbow(8), ylab = "count", main = "Overall sentiment analysis", las = 2)
```

#plot the graph to see the sentiment analysis for each topic
```{r}
emotions <-c("anger","anticipation","disgust","fear","joy","sadness","surprise","trust")

for(t in 1:5){
  sentimentdf_partial <- sentimentdf[sentimentdf$topic == t,] 
  
  WordsForSentiment = c()
  
  for(e in emotions ){
    
    value <- sum(sentimentdf_partial[,e])
    WordsForSentiment <- c(WordsForSentiment, value)
  }
  
  names(WordsForSentiment)  <-emotions
 
  totalSentimentWordTopic <- sum(WordsForSentiment)
  
  relativePercentage <- c()
  
  for(e in emotions){
    
    value <- WordsForSentiment[[e]]
    relativePercentage <- c(relativePercentage, value/totalSentimentWordTopic *100)
  }
  
  sentimentTopicdf <- data.frame(relativePercentage, emotions)
  
  sentimentTopicdf<- sentimentTopicdf %>% mutate(labels = percent(relativePercentage, scale = 1, accuracy = 0.01))
  
  print(ggplot(sentimentTopicdf, aes(x = "" , y = relativePercentage, fill = emotions)) +
  geom_col(width = 1, color = 1) +
  geom_text(aes(label = labels), position = position_stack(vjust = 0.5))+
  coord_polar(theta = "y") +
  theme_void()+
  ggtitle(paste0("Topic ", t)))
}
  



```

#plot the proprotion of emotions during the years
```{r}
sentimentdf%>%ungroup()
#angerbeforgroup <- count(sentimentdf, trust)
sentimentdf %>% group_by(year)
#angernumber <- count(sentimentdf, trust)

yearsvector <- rownames(table(sentimentdf$year))
#tableex
datalist <- data.frame(relativePercentage = integer(),emotions = character(),year =integer())
for (year in yearsvector){
  sentimentdf_partial_year <- sentimentdf[sentimentdf$year == year,]
  
  WordsForSentiment = c()
  
  for(e in emotions ){
    
    value <- sum(sentimentdf_partial_year[,e])
    WordsForSentiment <- c(WordsForSentiment, value)
  }
  
  names(WordsForSentiment)  <- emotions
 
  totalSentimentWordTopic <- sum(WordsForSentiment)
  
  relativePercentage <- c()
  
  for(e in emotions){
    
    value <- WordsForSentiment[[e]]
    relativePercentage <- c(relativePercentage, value/totalSentimentWordTopic *100)
  }
  
  sentimentTopicdf <- data.frame(relativePercentage, emotions, year)
  datalist<-rbind(datalist, sentimentTopicdf)
  
}

ggplot(doctopicclassification,mapping = aes(x= date, fill = topic, group=topic, colour = topic))+ geom_freqpoly()
  
  print(ggplot(datalist, aes(x = year , y = relativePercentage, fill = emotions, group = emotions, colour = emotions)) +
  geom_line())
  
```












Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
