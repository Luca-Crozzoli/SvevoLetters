---
title: "R Notebook"
output: html_notebook
---
##Step o library and dependency
```{r}
# loading the required packages
install.packages("dplyr")
install.packages("tidyr")
install.packages("tm")
install.packages("corpus")
install.packages("e1071")
install.packages("wordcloud2")
install.packages("caret")
install.packages("tidytext")
install.packages("tokenizers")
install.packages("textclean")
install.packages("stopwords")
install.packages("purrr")
install.packages("tibble")
install.packages("ggwordcloud")
install.packages("scales")#for method date_format
install.packages("lubridate")#fro floor_date method
install.packages("textmineR")

library(textstem)
library(textmineR)
library(scales)
library(lubridate)
library(ggwordcloud)
library(tibble)
library(purrr)
library(textclean)
library(dplyr)
library(tidyr)
library(textreg)
library(tm)
library(writexl)
library(corpus)
library(e1071)
library(wordcloud2)
library(caret)
library(tidytext)
library(tokenizers)
```

##Step 1 Load Data
```{r}
rawData <- read.csv(file = "C:/Users/User/Documents/UNITS/Magistrale/Primo Anno/1Â°semestre/Machine Learning/SvevoLetters/carteggiosvevo3.csv", sep = ";", header = TRUE)
head(rawData)
dimensions <- dim(rawData)
rows <- dimensions[1]
columns <- dimensions[2]
```
##Step 2 pre processing
```{r}
LetterCorpusSection <- table(rawData$corpus)
Sections <- dim(LetterCorpusSection)
Sections
SectionNames <-levels(factor(rawData$corpus))
SectionNames
Languages<-levels(factor(rawData$mainLanguage))
Languages
```


##New section in which we perform preprocessing
```{r}

rawData$date <- gsub("00","01",rawData$date)#for unknown month or year we assume it is the first month and first year
rawData$date

#converting the data as a character into a Date object 
rawData$date <- as.Date(rawData$date,tryFormats = c("%d-%m-%Y", "%d/%m/%Y"))
#taking in account only the Italian letters because they are the vast majority in the dataset
rawData<- subset(rawData, mainLanguage=="ITA")
```


##create the corpus composed only by the text
```{r}
myCorpus <- textclean::replace_non_ascii(rawData$text) %>%VectorSource()%>%VCorpus()

myCorpus<- tm_map(myCorpus, content_transformer(removeNumbers))

myRemovePunctuation <- function(x){gsub("([<>])|[[:punct:]]","",x)}
myCorpus <- tm_map(myCorpus,content_transformer(myRemovePunctuation))

myCorpus <- tm_map(myCorpus, content_transformer(tolower))

myStopwords <- c(stopwords("it"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

myCorpus<- tm_map(myCorpus, stripWhitespace)
myCorpus[[1]]$content
install.packages("udpipe")
library(udpipe)
ud_model <- udpipe_download_model(language = "italian")

ud_model <- udpipe_load_model(ud_model$file_model)
x <- udpipe_annotate(ud_model, x = rawData$text, doc_id = row.names(rawData))
x <- as.data.frame(x)

x$token_lower = tolower(x$token)
x$lemma_lower = tolower(x$lemma)
x <- subset(x, select = -c(token,lemma))
names(x)[names(x)=="token_lower"]<-"token" 
names(x)[names(x)=="lemma_lower"]<-"lemma"

lemmadictionary <- distinct(x[,c("token","lemma")],token, .keep_all = TRUE)

#lemmadictionary <- make_lemma_dictionary(rawData$text, engine = "hunspell")
lemmatizeStringpersonal <- function (x){
  lemmatize_strings(x,dictionary = lemmadictionary)
}

myCorpus <- tm_map(myCorpus, lemmatizeStringpersonal)
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myCorpus[[1]]
```

##Creation of the document term matrix and computing the cloud words to see the more frequent word in all the letters
```{r}

  bigramTokenizer <- 
    function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = "_"), use.names = FALSE)
trigramTokenizer <- 
    function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = "_"), use.names = FALSE)
  
#unigramdtm
unidtm <- DocumentTermMatrix(myCorpus)
unidtm <- removeSparseTerms(unidtm, 0.90)

#bigramdtm
  bidtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = bigramTokenizer ))
  bidtm <- removeSparseTerms(bidtm, 0.90)#considero i termini che appaion almeno nel 70% dei documenti
  
#trigramdtm
  tridtm <- DocumentTermMatrix(myCorpus, control = list(tokenize = trigramTokenizer ))
  tridtm <- removeSparseTerms(tridtm, 0.90)#considero i termini che appaion almeno nel 70% dei documenti
  


#dtm<-DocumentTermMatrix(myCorpus)
#inspect(unidtm)
m <- as.matrix(unidtm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(freq=v,word = names(v))
head(d, 10)
install.packages("wordcloud")
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

#inspect(bidtm)
m <- as.matrix(bidtm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(freq=v,word = names(v))
head(d, 10)
install.packages("wordcloud")
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

#inspect(tridtm)
m <- as.matrix(tridtm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(freq=v,word = names(v))
head(d, 10)
install.packages("wordcloud")
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
##Remove words thaht occur only in less 5 dcuments and the common words which appears on more than 90% of all documents
```{r}
word_freq <- findFreqTerms(unidtm,lowfreq = 1, highfreq = nrow(unidtm)*0.9)
unidtm <-unidtm[,word_freq]
m <- as.matrix(unidtm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(freq=v,word = names(v))
head(d, 10)
install.packages("wordcloud")
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
#inspect(unidtm)
```

##Evaluation fo the LDA model to find the optimal number of arguments, preparing the data frame related with coherenc and number of topics
```{r}
jaccard_similarity<- function(topic1,topic2){
  intersection = length(intersect(topic1,topic2))
  union = length(topic1)+length(topic2)- intersection
  return (intersection/union)
}

dtm_lda <- Matrix::Matrix(as.matrix(bidtm), sparse = T)
coherence = c()
similarity = c()
kvalues = seq(2,15,by=1)
for(k in kvalues){
  set.seed(123)
  m <- FitLdaModel(dtm =dtm_lda,
                        k = k, 
                        iterations = 500,
                        burnin = 400, 
                        calc_coherence = T
                        )
  letters_word_topic_iterative <- GetTopTerms(m$phi,30)%>%as.data.frame()%>%set_names(paste("topic",1:k))

  sims = c()
  for(t1 in 1:(k-1)){
    for(t2 in (t1+1):k ){
      topic1 <- as.vector(letters_word_topic_iterative[,c(t1)])
      topic2 <- as.vector(letters_word_topic_iterative[,c(t2)])
      sims <- c(sims, jaccard_similarity(topic1,topic2))                    
    }
  }

  similaritymean <- mean(sims)
  similarity <- c(similarity, similaritymean)
  
  cmean <- mean(m$coherence)
  coherence <- c(coherence, cmean)
}
data_graph_k_C_s <- data.frame(coherence,similarity,kvalues)
```

##visualizaation of coherance with teh number of topics
```{r}
data_graph_k_C_s <- data_graph_k_C_s %>%
  rowwise()%>%
  mutate(DiffCS = coherence - similarity)

##1K TOPICS SELECTOR-----------------------
#ideal_k = which(data_graph_k_C_s$coherence == max(coherence) & data_graph_k_C_s$similarity == min(similarity))

##2K TOPICS SELECTOR--------------------------
#ideal_k = as.integer(data_graph_k_C_s[data_graph_k_C_s$DiffCS == max(data_graph_k_C_s$DiffCS), "kvalues"])

##3K TOPICS SELECTOR------------------------
#Take the best 5 potentially k values and from the 5 we consider only 1 with the less kvalues
ideal_5_keys <- head(data_graph_k_C_s[order(-data_graph_k_C_s$DiffCS),],3)
ideal_k <- as.integer(ideal_5_keys[ideal_5_keys$kvalues == min(ideal_5_keys$kvalues), "kvalues"])

data_graph_k_C_s%>%
  gather(key,value,coherence,similarity)%>%
  ggplot(aes(x=kvalues, y=value, colour=key))+
  geom_line()+
  geom_vline(xintercept = ideal_k,color="red")

#ideal k 13/12/2021

#ggplot(data=data_graph_k_C, aes(x=kvalues,y=coherence)) +
 # geom_line() +
  #geom_point()+
  #geom_vline(xintercept = ideal_k,color="red")+
  #scale_x_continuous(breaks = seq(0, 10, 1)) +
  #labs(x = "Number of Topics", y = "Coherence", title = "Coherence Score over Number of Topics") +
  #theme_minimal() +
  #theme(panel.grid.minor = element_blank())

```
##Topic modelling with LDA LAtent Dirichlet Allocation
```{r}
install.packages("topicmodels")
library(topicmodels)
set.seed(123)

lda_news <- FitLdaModel(dtm =dtm_lda,
                        k = 9, 
                        iterations = 500,
                        burnin = 400, 
                        calc_coherence = T
                        )


```
##Below are some important attribute acquired from the LDA Model:

phi : Posterior per-topic-per-word probabilities
theta : Posterior per-document-per-topic probabilities
alpha : Prior per-document-per-topic probabilities
beta : Prior per-document-per-topic probabilities
coherence : The probabilistic coherence of each topic

If a term has a high value of theta, it has a high probability of that term being generated from that topic. This also indicates that the term has a high association toward a certain topic.
```{r}
lda_news$phi%>% rowSums()

letters_word_topic <- GetTopTerms(lda_news$phi,30)%>% as.data.frame()%>%set_names(paste("topic",1:9))
View(letters_word_topic)


```


##grphical way to reppresents topic with related words using word cloud
```{r}
letters_word_topic %>% 
   rownames_to_column("id") %>%
   mutate(id = as.numeric(id)) %>% 
   pivot_longer(-id, names_to = "topic", values_to = "term") %>% 
   ggplot(aes(label = term, size = rev(id), color = topic, alpha = rev(id))) +
   geom_text_wordcloud(seed = 123) +
   facet_wrap(~topic, scales = "free") +
   scale_alpha_continuous(range = c(0.4, 1)) +
   #scale_color_manual(values = c( "dodgerblue4", "firebrick4", "darkgreen")) +
   theme_minimal() +
   theme(strip.background = element_rect(fill = "firebrick"),
         strip.text.x = element_text(colour = "white"))


```

#most related document for topic 3 we use the theta parameter from the lda
```{r}
#letters_doc_topic <- lda_news$theta %>% 
#   as.data.frame() %>% 
#   rownames_to_column("id")#%>%
#   #mutate(id = as.numeric(id))


#letters_doc_topic %>% 
#   arrange(desc(t_3))%>%
#   left_join(rawData%>%
#               mutate(n=as.character(n))%>%
#               select(n,corpus,date,sender,recipient,text), 
#               by=c("id"= "n"))%>% 
#  select(id,date,sender,recipient,t_3)%>%
#  view()
```

#Top topic for each document
```{r}

doctoptopic <- lda_news$theta %>% 
   as.data.frame()


colnames(doctoptopic) <- c(1,2,3,4,5,6,7,8,9)

toptopics <- as.data.frame(cbind(id=row.names(rawData), topic= apply(doctoptopic,1,function(x)names(doctoptopic)[which.max(x)])))
toptopics["date"] <- rawData$date


transform(toptopics, topic = as.numeric(topic))


ggplot(toptopics,mapping = aes(x= date, fill = topic, group=topic, colour = topic))+ geom_freqpoly() + facet_wrap(facets = vars(topic)) + theme(legend.position = "none")

```
##Topic proportion over the time
```{r}
#we will see the range of date 
range(rawData$date)
letters_doc_topic3 <- lda_news$theta %>% 
   as.data.frame()
letters_doc_topic3["date"] <- rawData$date


#we plot the topic proportion over the time
#table_topic_date <-letters_doc_topic %>% 
#   left_join(   rawData %>% 
#                mutate(n = as.character(n)) %>% 
#                select(n,date), 
#                by = c("id" = "n")) %>% 
#   select(id, everything()) 
   
df_topic_date <-letters_doc_topic3%>%pivot_longer(c(t_1, t_2, t_3, t_4), names_to = "topic", values_to = "theta") %>% 
         mutate(topic = case_when( topic == "t_1" ~ "TOPIC1",
                             topic == "t_2" ~ "TOPIC2",
                             topic == "t_3"~ "TOPIC3",
                             TRUE ~ "TOPIC4") %>% 
          factor(levels = c("TOPIC2", "TOPIC1", "TOPIC3","TOPIC4")),
          time = floor_date(date, unit = "month")) %>% 
   group_by(time, topic)%>%
   summarise(theta = mean(theta)) 
df_topic_date


df_topic_date%>%ggplot(aes(time, theta, fill = topic, color = topic)) +
   #geom_area()+
   geom_line() +
   theme_minimal() +
   theme(legend.position = "top") +
  # scale_x_date(date_breaks = "1 years", 
                #labels = date_format(format = "%Y-%m-%d")) +
  # scale_y_continuous() +
   #scale_fill_manual(values = c("firebrick", "orange", "dodgerblue3", "green")) +
   #labs(x = NULL, y = expression(theta), color = NULL, 
        #title = "Topic Proportions Over Time on daily Interval")+
  
  facet_wrap(facets = vars(topic))
```




##Topic proportion in sender and receiver
```{r}
install.packages("scales")#for method date_format
library(scales)
#we will see the range of date 
range(rawData$date)

#Table with id_ t1,t2,t3
sender_receiver <-letters_doc_topic %>% 
   left_join(   rawData %>% 
                mutate(n = as.character(n)) %>% 
                select(n,sender,recipient), 
                by = c("id" = "n")) %>% 
   select(id, everything())

view(sender_receiver)

sender_receiver%>%pivot_longer(c(t_1, t_2, t_3), names_to = "topic", values_to = "theta") %>% 
   mutate(topic = case_when( topic == "t_1" ~ "TOPIC1",
                             topic == "t_2" ~ "TOPIC2",
                             TRUE ~ "TOPIC3") %>% 
          factor(levels = c("TOPIC2", "TOPIC1", "TOPIC3")),
          Senders = sender,
          Receivers = recipient )%>%
  group_by(sender,recipient,topic)


plot1 <-ggplot(sender_receiver, aes(x=sender, y=recipient)) + 
  geom_point(aes(size=t_1, color="red"))
print(plot1)

plot2 <-ggplot(sender_receiver, aes(x=sender, y=recipient)) + 
  geom_point(aes(size=t_2, color="green"))
print(plot2)

plot3 <-ggplot(sender_receiver, aes(x=sender, y=recipient)) + 
  geom_point(aes(size=t_3, color="blue"))
print(plot3)
```


#Trying LDA vis
```{r}

```


















































Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
