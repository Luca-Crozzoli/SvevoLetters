---
title: "R Notebook"
output: html_notebook
---
##Step o library and dependency
```{r}
# loading the required packages
install.packages("dplyr")
install.packages("tidyr")
install.packages("tm")
install.packages("corpus")
install.packages("e1071")
install.packages("wordcloud2")
install.packages("caret")
install.packages("tidytext")
install.packages("tokenizers")
install.packages("textclean")
install.packages("stopwords")

library(textclean)
library(dplyr)
library(tidyr)
library(textreg)
library(tm)
library(writexl)
library(corpus)
library(e1071)
library(wordcloud2)
library(caret)
library(tidytext)
library(tokenizers)
```

##Step 1 Load Data
```{r}
df <- read.csv(file = "C:/Users/User/Documents/UNITS/Magistrale/Primo Anno/1Â°semestre/Machine Learning/SvevoLetters/carteggiosvevoShort80.csv", sep = ";", header = TRUE)
head(df)
dimensions <- dim(df)
rows <- dimensions[1]
columns <- dimensions[2]
```
##Step 2 pre processing
```{r}
LetterCorpusSection <- table(df[,2])
Sections <- dim(LetterCorpusSection)
Sections
SectionNames <-levels(factor(df$corpus))
SectionNames
```


##New section in which we perform preprocessing
```{r}

df$date <- gsub("00","01",df$date)#forunknown month or year we assume it is the first month and first year
df$date

#converting the data as a character into a Date object 
df$date <- as.Date(df$date,tryFormats = c("%d-%m-%Y", "%d/%m/%Y"))


```


##create a copus composed only by the text data
```{r}
myCorpus <-VCorpus(VectorSource(textclean::replace_non_ascii(df$text)))

```
##removing Numbers
```{r}
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus
```
##removing Punctuations
```{r}
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus[[36]][["content"]]
```
##convertgin to lower case
```{r}
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus[[36]][["content"]]
```
##removing stop words
```{r}
myStopwords <- c(stopwords("it"),stopwords("en"), stopwords("de"),stopwords("fr"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus[[36]][["content"]]
```
## remove extra whitespaces
```{r}
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus[[36]][["content"]]
```


##Tokenization and stemming function
```{r}
library(hunspell)
stem_hunspell <- function(term) {
    # look up the term in the dictionary
    stems <- hunspell_stem(term)[[1]]
    
    if (length(stems) == 0) { # if there are no stems, use the original term
        stem <- term
    } else { # if there are multiple stems, use the last one
        stem <- stems[[length(stems)]]
    }
    return(stem)
}
    
#CharacterCorpus <- convert.tm.to.character(myCorpus)
myCorpus%>% text_tokens(stemmer=stem_hunspell)
dtm<-TermDocumentMatrix(myCorpus)
inspect(dtm)
```
##stemming function
```{r}
words <- sort(table(letter_terms), decreasing = TRUE)
print(head(words, 30))

#Removing stop words from all the words
df_words <- as.data.frame(words)
df_words$stems <- as.character(df_words$stems)
stops <- df_words$stems %in% stopwords(kind = "en")
wcdata <- head(df_words[!stops,], 150)
print(wcdata, max = 40)

```

```{r}
dtm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
install.packages("wordcloud")
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
