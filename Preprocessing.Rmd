---
title: "R Notebook"
output: html_notebook
---
##Step o library and dependency
```{r}
# loading the required packages
install.packages("dplyr")
install.packages("tidyr")
install.packages("tm")
install.packages("corpus")
install.packages("e1071")
install.packages("wordcloud2")
install.packages("caret")
install.packages("tidytext")
install.packages("tokenizers")
install.packages("textclean")
install.packages("stopwords")
install.packages("purrr")
install.packages("tibble")
install.packages("ggwordcloud")

library(ggwordcloud)
library(tibble)
library(purrr)
library(textclean)
library(dplyr)
library(tidyr)
library(textreg)
library(tm)
library(writexl)
library(corpus)
library(e1071)
library(wordcloud2)
library(caret)
library(tidytext)
library(tokenizers)
```

##Step 1 Load Data
```{r}
df <- read.csv(file = "C:/Users/User/Documents/UNITS/Magistrale/Primo Anno/1Â°semestre/Machine Learning/SvevoLetters/carteggiosvevoShort80.csv", sep = ";", header = TRUE)
head(df)
df$n <- df$n +1
dimensions <- dim(df)
rows <- dimensions[1]
columns <- dimensions[2]
```
##Step 2 pre processing
```{r}
LetterCorpusSection <- table(df[,2])
Sections <- dim(LetterCorpusSection)
Sections
SectionNames <-levels(factor(df$corpus))
SectionNames
```


##New section in which we perform preprocessing
```{r}

df$date <- gsub("00","01",df$date)#forunknown month or year we assume it is the first month and first year
df$date

#converting the data as a character into a Date object 
df$date <- as.Date(df$date,tryFormats = c("%d-%m-%Y", "%d/%m/%Y"))


```


##create a copus composed only by the text data removing non ascii characters
```{r}
textclean::replace_non_ascii(df$text)
myCorpus2 <- as_corpus_frame(df)

myCorpus <-VCorpus(VectorSource(textclean::replace_non_ascii(df$text)))

```
##removing Numbers
```{r}
myCorpus<- tm_map(myCorpus, removeNumbers)
myCorpus
```
##removing Punctuations
```{r}
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus[[36]][["content"]]
```
##converting to lower case
```{r}
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus[[36]][["content"]]
```
##removing stop words (italian french endglish and german)
```{r}
myStopwords <- c(stopwords("it"),stopwords("en"), stopwords("de"),stopwords("fr"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus[[36]][["content"]]
```
## remove extra whitespaces
```{r}
myCorpus<- tm_map(myCorpus, stripWhitespace)
myCorpus[[36]][["content"]]
```


##Tokenization into words and stemming function
```{r}
library(hunspell)
stem_hunspell <- function(term) {
    # look up the term in the dictionary
    stems <- hunspell_stem(term)[[1]]
    
    if (length(stems) == 0) { # if there are no stems, use the original term
        stem <- term
    } else { # if there are multiple stems, use the last one
        stem <- stems[[length(stems)]]
    }
    return(stem)
}
myCorpus%>% text_tokens(stemmer=stem_hunspell)
myCorpus

```

##Creation fo the document term matrix and computing the cloud words to see the more frequent word in all the letters
```{r}
dtm<-DocumentTermMatrix(myCorpus)
inspect(dtm)
m <- as.matrix(dtm)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(freq=v,word = names(v))
head(d, 10)
install.packages("wordcloud")
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
##Remove words thaht occur only in less 5 dcuments and the common words which appears on more than 90% of all documents
```{r}
word_freq <- findFreqTerms(dtm,lowfreq = 5, highfreq = nrow(dtm)*0.9)
dtm <-dtm[,word_freq]
inspect(dtm)
```

##Topic modelling with LDA LAtent Dirichlet Allocation
```{r}
install.packages("textmineR")
library(textmineR)
dtm_lda <- Matrix::Matrix(as.matrix(dtm), sparse = T)

set.seed(123)
lda_news <- FitLdaModel(dtm = dtm_lda, 
                        k = 3, 
                        iterations = 5000,
                        burnin = 4000, 
                        calc_coherence = T
                        )
```
##Below are some important attribute acquired from the LDA Model:

phi : Posterior per-topic-per-word probabilities
theta : Posterior per-document-per-topic probabilities
alpha : Prior per-document-per-topic probabilities
beta : Prior per-document-per-topic probabilities
coherence : The probabilistic coherence of each topic

If a term has a high value of theta, it has a high probability of that term being generated from that topic. This also indicates that the term has a high association toward a certain topic.
```{r}
lda_news$phi%>% rowSums()
letters_word_topic <- GetTopTerms(lda_news$phi,30)%>% as.data.frame()%>%set_names(paste("topic",1:3))
View(letters_word_topic)


```


##grphical way
```{r}
letters_word_topic %>% 
   rownames_to_column("id") %>%
   mutate(id = as.numeric(id)) %>% 
   pivot_longer(-id, names_to = "topic", values_to = "term") %>% 
   ggplot(aes(label = term, size = rev(id), color = topic, alpha = rev(id))) +
   geom_text_wordcloud(seed = 123) +
   facet_wrap(~topic, scales = "free") +
   scale_alpha_continuous(range = c(0.4, 1)) +
   scale_color_manual(values = c( "dodgerblue4", "firebrick4", "darkgreen")) +
   theme_minimal() +
   theme(strip.background = element_rect(fill = "firebrick"),
         strip.text.x = element_text(colour = "white"))


```
##Document topic probabilities
#most related document for topic 1
```{r}
letters_doc_topic <- lda_news$theta %>% 
   as.data.frame() %>% 
   rownames_to_column("id")

lapply(letters_doc_topic$id, as.numeric)

letters_doc_topic %>% 
   arrange(desc(t_1))%>%
   left_join(df%>%
               mutate(n=as.character(n))%>%
               select(n,corpus,date,sender,recipient,text), 
               by=c("id"= "n"))%>% 
  select(id,date,sender,recipient,t_1)%>%
  view()
```

#most related document for topic 2
```{r}
letters_doc_topic <- lda_news$theta %>% 
   as.data.frame() %>% 
   rownames_to_column("id")

lapply(letters_doc_topic$id, as.numeric)

letters_doc_topic %>% 
   arrange(desc(t_2))%>%
   left_join(df%>%
               mutate(n=as.character(n))%>%
               select(n,corpus,date,sender,recipient,text), 
               by=c("id"= "n"))%>% 
  select(id,date,sender,recipient,t_2)%>%
  view()
```

#most related document for topic 3
```{r}
letters_doc_topic <- lda_news$theta %>% 
   as.data.frame() %>% 
   rownames_to_column("id")

lapply(letters_doc_topic$id, as.numeric)

letters_doc_topic %>% 
   arrange(desc(t_3))%>%
   left_join(df%>%
               mutate(n=as.character(n))%>%
               select(n,corpus,date,sender,recipient,text), 
               by=c("id"= "n"))%>% 
  select(id,date,sender,recipient,t_3)%>%
  view()
```






























































Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
