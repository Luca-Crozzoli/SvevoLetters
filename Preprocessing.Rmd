---
title: "R Notebook"
output: html_notebook
---
##Step o library and dependency
```{r}
# loading the required packages
install.packages("dplyr")
install.packages("tidyr")
install.packages("textreg")
install.packages("tm")
install.packages("writexl")
install.packages("corpus")
install.packages("e1071")
install.packages("wordcloud2")
install.packages("caret")
install.packages("tidytext")
install.packages("tokenizers")
install.packages("textclean")

library(textclean)
library(dplyr)
library(tidyr)
library(textreg)
library(tm)
library(writexl)
library(corpus)
library(e1071)
library(wordcloud2)
library(caret)
library(tidytext)
library(tokenizers)
```

##Step 1 Load Data
```{r}
df <- read.csv(file = "C:/Users/User/Documents/UNITS/Magistrale/Primo Anno/1Â°semestre/Machine Learning/SvevoLetters/carteggiosvevo3.csv", sep = ";", header = TRUE)
head(df)
dimensions <- dim(df)
rows <- dimensions[1]
columns <- dimensions[2]
```
##Step 2 pre processing
```{r}
LetterCorpusSection <- table(df[,2])
Sections <- dim(LetterCorpusSection)
Sections
SectionNames <-levels(factor(df$corpus))
SectionNames
```
##Updatying an eglish list to compute the stemming


##New section in which we perform preprocessing
```{r}

df$date <- gsub("00","01",df$date)#forunknown month or year we assume it is the first month and first year
df$date

#converting the data as a character into a Date object 
df$date <- as.Date(df$date,tryFormats = c("%d-%m-%Y", "%d/%m/%Y"))
# verifying
df$date

```

##OLD things
```{r}
documents_length <- sapply(strsplit(df_clean$text, " "), length)
documents_length

documents_length %>% 
   summary()
```

##create a copus composed only by the text data
```{r}
myCorpus <- VCorpus(VectorSource(df$text))
myCorpus[[50]][["content"]]

```
##removing Numbers
```{r}
myCorpus <- tm_map(myCorpus,content_transformer(removeNumbers))
myCorpus[[50]][["content"]]
```
##removing Punctuations
```{r}
removeNumPunct <- function(x) gsub("([<>])|[[:punct:]]", " ", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus[[50]][["content"]]
```
##convertgin to lower case
```{r}
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus[[50]][["content"]]
```
##removing stop words
```{r}
myStopwords <- c(stopwords(kind = 'SMART'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus[[50]][["content"]]
```
## remove extra whitespaces
```{r}
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus[[50]][["content"]]
```

##Conversion of vectors into character to allow tokenization
```{r}
char_corpus <- convert.tm.to.character(myCorpus)
char_corpus

myCorpus[[50]][["content"]]
```
##stemming function
```{r}
library(hunspell)
stem_hunspell <- function(term) {
    # look up the term in the dictionary
    stems <- hunspell_stem(term)[[1]]
    
    if (length(stems) == 0) { # if there are no stems, use the original term
        stem <- term
    } else { # if there are multiple stems, use the last one
        stem <- stems[[length(stems)]]
    }
    return(stem)
}
#counting word occourency correpsonding to a steam
allwords <- hunspell_parse(char_corpus, format = "text")
stems <- unlist(stem_hunspell(unlist(allwords)))
words <- sort(table(stems), decreasing = TRUE)
print(head(words, 30))

#Removing stop words from all the words
df_words <- as.data.frame(words)
df_words$stems <- as.character(df_words$stems)
stops <- df_words$stems %in% stopwords(kind = "en")
wcdata <- head(df_words[!stops,], 150)
print(wcdata, max = 40)

#Plot the cloud of words based on the frequency
install.packages("wordcloud")
library(wordcloud2)
names(wcdata) <- c("word", "freq")
wcdata$freq <- (wcdata$freq)^(2/3)
wordcloud2(wcdata)
```

```{r}
dtm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
install.packages("wordcloud")
library(wordcloud)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
